{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3435d170-3b35-49a7-8206-a9b828e17651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 00:38:20.491468: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-29 00:38:20.523553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 00:38:21.054224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa.display, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa.display, os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from skimage.transform import resize\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "mel_train = []\n",
    "cqt_train = []\n",
    "mel_test = []\n",
    "cqt_test = []\n",
    "label_train = []\n",
    "label_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb5d5ab-5d8f-423d-9756-75a58c118dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(audio_folder):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(audio_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "                labels.append(os.path.basename(root))\n",
    "\n",
    "    return audio_files, labels\n",
    "def process_audio(file_path):\n",
    "    sample_rate = 32000\n",
    "    n_mfcc = 13\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "\n",
    "    # Load audio file\n",
    "    audio, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=128)\n",
    "    # Compute MFCC from the Mel spectrogram\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=n_mfcc)\n",
    "\n",
    "    # Resize spectrograms\n",
    "    mfcc_resized = resize(mfcc, (224, 224, 3), mode='reflect', anti_aliasing=True)\n",
    "    mel_resized = resize(mel, (224, 224, 3), mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    # Clear memory\n",
    "    del audio\n",
    "\n",
    "    return mfcc_resized, mel_resized\n",
    "def process_audio_files(audio_files, audio_folder):\n",
    "    mfcc_train = []\n",
    "    mel_train = []\n",
    "    y_train = []\n",
    "\n",
    "    total_files = len(audio_files)\n",
    "\n",
    "    with tqdm(total=total_files, ncols=80) as pbar:\n",
    "        for file_path in audio_files:\n",
    "            if file_path.endswith(\".wav\"):\n",
    "                # Process audio and extract spectrograms\n",
    "                mfcc, mel = process_audio(file_path)\n",
    "                # Append spectrograms and label to the train arrays\n",
    "                mfcc_train.append(mfcc)\n",
    "                mel_train.append(mel)\n",
    "                # Extract label from subfolder name\n",
    "                label = os.path.basename(os.path.dirname(file_path))\n",
    "                y_train.append(label)\n",
    "                pbar.set_postfix({'file': file_path})\n",
    "                pbar.update()\n",
    "\n",
    "    # Convert train arrays to NumPy arrays\n",
    "    mfcc_train = np.array(mfcc_train)\n",
    "    mel_train = np.array(mel_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return mfcc_train, mel_train, y_train\n",
    "def main(audio_folder):\n",
    "    audio_files, labels = load_audio_files(audio_folder)\n",
    "    X_mfcc, X_mel, y = process_audio_files(audio_files, audio_folder)\n",
    "    return X_mfcc, X_mel, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacc1f33-14f9-4eb5-8371-d41d9bed4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 514/514 [00:12<00:00, 40.81it/s, file=../data/raw/tired/volume_adjustmen\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "audio_folder = '../data/raw/'\n",
    "mfcc_test, mel_test, label_test = main(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c243007c-ec0d-4adf-84a0-a86a7ccfa12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 514/514 [00:13<00:00, 39.52it/s, file=../data/raw/tired/volume_adjustmen\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "audio_folder = '../data/raw/'\n",
    "mfcc_train, mel_train, label_train = main(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cd7e35e-3030-4360-9869-539b3879cdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Apply RandomOverSampler to balance the classes in the training set\u001b[39;00m\n\u001b[1;32m     30\u001b[0m ros \u001b[38;5;241m=\u001b[39m RandomOverSampler(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m ros\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train_encoded)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute class weights to account for the imbalance during evaluation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y_train_encoded), y_train_encoded)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on your label data\n",
    "label_encoder.fit(label_train)\n",
    "\n",
    "# Transform the labels to numerical values\n",
    "y_train_encoded = label_encoder.transform(label_train)\n",
    "y_test_encoded = label_encoder.transform(label_test)\n",
    "\n",
    "# Get the unique class names\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Create a dictionary to map numerical labels to class names\n",
    "label_to_class = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Convert the encoded labels to one-hot encoded vectors\n",
    "label_train_encoded = to_categorical(y_train_encoded)\n",
    "label_test_encoded = to_categorical(y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0c85d8-5fe3-4ef1-a530-8564d26252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "mel_train_norm = np.array(mel_train) / 255  \n",
    "mel_test_norm = np.array(mel_test) / 255      \n",
    "mfcc_train_norm = np.array(mfcc_train) / 255    \n",
    "mfcc_test_norm = np.array(mfcc_test) / 255    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f41a1608-a6fa-4ea6-9ba8-32b50e941b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "mel_input = Input(shape=(224, 224, 3))  # Modify the shape according to your mel spectrogram dimensions\n",
    "cqt_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Convolutional layers for Mel spectrograms\n",
    "mel_conv = Conv2D(32, kernel_size=(3, 3), activation='relu')(mel_input)\n",
    "mel_conv = MaxPooling2D(pool_size=(2, 2))(mel_conv)\n",
    "mel_conv = Conv2D(64, kernel_size=(3, 3), activation='relu')(mel_conv)\n",
    "mel_conv = MaxPooling2D(pool_size=(2, 2))(mel_conv)\n",
    "mel_conv = Conv2D(128, kernel_size=(3, 3), activation='relu')(mel_conv)\n",
    "mel_conv = MaxPooling2D(pool_size=(2, 2))(mel_conv)\n",
    "mel_conv = Flatten()(mel_conv)\n",
    "\n",
    "# Convolutional layers for CQT spectrograms\n",
    "cqt_conv = Conv2D(32, kernel_size=(3, 3), activation='relu')(cqt_input)\n",
    "cqt_conv = MaxPooling2D(pool_size=(2, 2))(cqt_conv)\n",
    "cqt_conv = Conv2D(64, kernel_size=(3, 3), activation='relu')(cqt_conv)\n",
    "cqt_conv = MaxPooling2D(pool_size=(2, 2))(cqt_conv)\n",
    "cqt_conv = Conv2D(128, kernel_size=(3, 3), activation='relu')(cqt_conv)\n",
    "cqt_conv = MaxPooling2D(pool_size=(2, 2))(cqt_conv)\n",
    "cqt_conv = Flatten()(cqt_conv)\n",
    "\n",
    "# Merge the branches\n",
    "merged = concatenate([mel_conv, cqt_conv])\n",
    "\n",
    "# Dense layers for classification\n",
    "dense = Dense(256, activation='relu')(merged)\n",
    "dense = Dropout(0.5)(dense)  # Add dropout with a rate of 0.5\n",
    "dense = Dense(128, activation='relu')(dense)\n",
    "dense = Dropout(0.5)(dense)  # Add dropout with a rate of 0.5\n",
    "output = Dense(5, activation='softmax')(dense)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[mel_input, cqt_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03949f55-b6cb-46d1-86d6-323583445a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping callback\n",
    "earlystop_callback = EarlyStopping(\n",
    "    monitor='val_accuracy', patience=5, mode='max', verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca43ab5-9737-4037-b9e0-0e01541d0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 939ms/step - accuracy: 0.2133 - loss: 2.0326 - val_accuracy: 0.3132 - val_loss: 1.4761\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 949ms/step - accuracy: 0.3560 - loss: 1.4601 - val_accuracy: 0.4669 - val_loss: 1.2069\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 957ms/step - accuracy: 0.3941 - loss: 1.3130 - val_accuracy: 0.6362 - val_loss: 0.9974\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 965ms/step - accuracy: 0.5092 - loss: 1.1991 - val_accuracy: 0.6809 - val_loss: 0.8705\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 967ms/step - accuracy: 0.5896 - loss: 1.0232 - val_accuracy: 0.7840 - val_loss: 0.6756\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 955ms/step - accuracy: 0.7103 - loss: 0.8867 - val_accuracy: 0.8813 - val_loss: 0.6077\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 958ms/step - accuracy: 0.7097 - loss: 0.7737 - val_accuracy: 0.9222 - val_loss: 0.4195\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 957ms/step - accuracy: 0.8189 - loss: 0.5871 - val_accuracy: 0.9553 - val_loss: 0.2631\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 971ms/step - accuracy: 0.8625 - loss: 0.4165 - val_accuracy: 0.9611 - val_loss: 0.2107\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 959ms/step - accuracy: 0.8613 - loss: 0.3203 - val_accuracy: 0.9747 - val_loss: 0.1038\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 961ms/step - accuracy: 0.9400 - loss: 0.1971 - val_accuracy: 0.9669 - val_loss: 0.0911\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 987ms/step - accuracy: 0.9327 - loss: 0.2847 - val_accuracy: 0.9883 - val_loss: 0.0520\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 953ms/step - accuracy: 0.9324 - loss: 0.2775 - val_accuracy: 0.9883 - val_loss: 0.0557\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 960ms/step - accuracy: 0.9639 - loss: 0.1399 - val_accuracy: 0.9903 - val_loss: 0.0652\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 958ms/step - accuracy: 0.9530 - loss: 0.1186 - val_accuracy: 0.9883 - val_loss: 0.0332\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 949ms/step - accuracy: 0.9673 - loss: 0.1198 - val_accuracy: 0.9864 - val_loss: 0.0431\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 944ms/step - accuracy: 0.9819 - loss: 0.0796 - val_accuracy: 0.9883 - val_loss: 0.0283\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 962ms/step - accuracy: 0.9551 - loss: 0.1830 - val_accuracy: 0.9961 - val_loss: 0.0331\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 959ms/step - accuracy: 0.9606 - loss: 0.2107 - val_accuracy: 0.9961 - val_loss: 0.0264\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 946ms/step - accuracy: 0.9807 - loss: 0.0727 - val_accuracy: 0.9942 - val_loss: 0.0454\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 950ms/step - accuracy: 0.9699 - loss: 0.1046 - val_accuracy: 0.9922 - val_loss: 0.0241\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 967ms/step - accuracy: 0.9819 - loss: 0.0544 - val_accuracy: 0.9942 - val_loss: 0.0183\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 967ms/step - accuracy: 0.9902 - loss: 0.0445 - val_accuracy: 0.9961 - val_loss: 0.0164\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([mel_train_norm, mfcc_train_norm], label_train_encoded,\n",
    "           batch_size=32, epochs=100,\n",
    "          validation_data=([mel_test_norm, mfcc_test_norm], label_test_encoded),\n",
    "          callbacks=[earlystop_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b49af6-2ef8-484c-9fb4-6612a76460cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7afdc00bcc10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4df9aff-4254-46b0-a136-bf4818262605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 161ms/step\n",
      "Accuracy: 0.9961089494163424\n",
      "Precision: 0.9961727371308285\n",
      "Recall: 0.9961089494163424\n",
      "F1 Score: 0.9961090984685339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict([mel_test_norm, mfcc_test_norm])\n",
    "\n",
    "# Convert the predictions from one-hot encoded format to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(label_test_encoded, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "precision = precision_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "recall = recall_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8571bd1-093f-468f-b62b-770b87b122e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted class label: hungry\n"
     ]
    }
   ],
   "source": [
    "# Load the audio file\n",
    "audio_file_path = '/home/kotaro/Music/babyTest.wav'\n",
    "#audio_file_path = '../data/raw/discomfort/noise_injection_999bf14b-e417-4b44-b746-9253f81efe38-1430844958178-1.7-m-04-ch_augmented.wav'\n",
    "\n",
    "\n",
    "# Process the audio to extract features\n",
    "mfcc, mel = process_audio(audio_file_path)\n",
    "\n",
    "# Normalize the features\n",
    "mfcc_norm = np.array([mfcc]) / 255\n",
    "mel_norm = np.array([mel]) / 255\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.predict([mel_norm, mfcc_norm])\n",
    "\n",
    "# Convert prediction from one-hot encoded to class label\n",
    "predicted_label_index = np.argmax(prediction)\n",
    "predicted_class_label = label_to_class[predicted_label_index]\n",
    "\n",
    "# Display the predicted class label\n",
    "print(\"Predicted class label:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ef639d1-7ff9-4fa0-91dc-821a84276da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: burping, Count: 40\n",
      "Class: hungry, Count: 139\n",
      "Class: belly_pain, Count: 80\n",
      "Class: discomfort, Count: 135\n",
      "Class: tired, Count: 120\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each class label in the original dataset\n",
    "class_counts = Counter(label_train)\n",
    "\n",
    "# Print the class counts\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"Class: {class_name}, Count: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
